# app.py
# Streamlit‚Äë–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: XGBoost + SHAP, –ø–æ—à–∞–≥–æ–≤—ã–π –≤—ã–≤–æ–¥
# ------------------------------------------------------
import streamlit as st, warnings, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from sklearn.model_selection import TimeSeriesSplit, cross_val_score
from sklearn.inspection import PartialDependenceDisplay
from sklearn.pipeline import Pipeline
from xgboost import XGBRegressor
import shap, os

warnings.filterwarnings("ignore")
sns.set_style("whitegrid")
plt.rcParams["figure.dpi"] = 110
st.set_page_config(page_title="Eco‚ÄëHealth¬†XGB", layout="wide")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ UI ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.title("–≠–∫–æ–ª–æ–≥–∏—è ‚Üí –∑–∞–±–æ–ª–µ–≤–∞–µ–º–æ—Å—Ç—å¬†:  XGBoost¬†+¬†SHAP")

upl = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ .xlsx –∏–ª–∏ .csv", type=("xlsx", "csv"))
if not upl:
    st.stop()

df = pd.read_csv(upl) if upl.name.lower().endswith(".csv") else pd.read_excel(upl, engine="openpyxl")
year_col = st.selectbox("–ö–æ–ª–æ–Ω–∫–∞ –≥–æ–¥–∞", sorted([c for c in df.columns if df[c].dtype != "object"]))
region_col = st.selectbox("–ö–æ–ª–æ–Ω–∫–∞ —Ä–∞–π–æ–Ω–∞", sorted(df.select_dtypes("object").columns))
target_col = st.selectbox("–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è", sorted([c for c in df.columns if c not in [year_col, region_col]]))

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
min_nonmiss = st.slider("–ú–∏–Ω. –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞", 0.0, 1.0, 0.3, 0.05)
min_row_fill = st.slider("–ú–∏–Ω. –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç—å —Å—Ç—Ä–æ–∫–∏ (–¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ä–∞–π–æ–Ω–∞)", 0.0, 1.0, 0.3, 0.01)
test_years = st.slider("–õ–µ—Ç –≤ —Ç–µ—Å—Ç–µ", 1, 3, 1)
max_depth = st.slider("max_depth", 2, 10, 3)
n_estim = st.slider("n_estimators", 100, 300, 100, 50)

if not st.button("üöÄ –ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–Ω–∞–ª–∏–∑"):
    st.stop()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –∫—ç—à ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
@st.cache_resource(show_spinner=False)
def train_cache(df, year_col, region_col, target_col,
                min_nonmiss, min_row_fill, test_years, max_depth, n_estim):
    # —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç–∏ —Å—Ç—Ä–æ–∫ (–º–∏–Ω–∏–º—É–º 30%)
    df = df[df.notna().mean(axis=1) >= min_row_fill]
    
    eco_cols = [c for c in df.columns if c.startswith(("Air_", "Water_", "Soil_"))]
    for c in eco_cols:
        lag = f"{c}_lag1"
        if lag not in df.columns:
            df[lag] = df.groupby(region_col)[c].shift(1)

    num_cols  = eco_cols + [f"{c}_lag1" for c in eco_cols]
    good_cols = [c for c in num_cols if df[c].notna().mean() >= min_nonmiss]

    # –≤—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ —Ä–∞–π–æ–Ω–∞–º
    num_districts = len(df[region_col].unique())
    # –ü–æ–∫–∞–∑–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö
    num_rows = len(df)
    st.write(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö: {num_rows}")

    # –ü–æ–∫–∞–∑–∞—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ä–∞–π–æ–Ω–æ–≤
    st.write(f"–í—ã–±—Ä–∞–Ω–æ {len(good_cols)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ {len(df[region_col].unique())} —Ä–∞–π–æ–Ω–æ–≤.")


    train_mask = df[year_col] < df[year_col].max() - test_years
    X_train = df.loc[train_mask, good_cols + [region_col]]
    y_train = df.loc[train_mask, target_col]
    X_test  = df.loc[~train_mask, good_cols + [region_col]]
    y_test  = df.loc[~train_mask, target_col]

    pipe = Pipeline([  # —Å–æ–∑–¥–∞—ë–º –ø–∞–π–ø–ª–∞–π–Ω —Å XGBoost
        ("prep", ColumnTransformer(
            [("cat", OneHotEncoder(handle_unknown="ignore"),
              [X_train.columns.get_loc(region_col)])],
            remainder="passthrough", sparse_threshold=0.3)),
        ("xgb", XGBRegressor(
            n_estimators=n_estim, max_depth=max_depth,
            learning_rate=0.045, subsample=0.8,
            colsample_bytree=0.8, reg_lambda=1.0,
            random_state=42, n_jobs=-1, missing=np.nan))
    ]).fit(X_train, y_train)

    return df, good_cols, pipe, X_train, y_train, X_test, y_test

with st.spinner("‚è≥ –û–±—É—á–∞–µ–º XGBoost‚Ä¶"):
    df, good_cols, pipe, X_tr, y_tr, X_te, y_te = train_cache(
        df.copy(), year_col, region_col, target_col,
        min_nonmiss, min_row_fill, test_years, max_depth, n_estim)
st.success("‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞")

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ú–µ—Ç—Ä–∏–∫–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

y_pred = pipe.predict(X_te)

# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ R¬≤, RMSE –∏ MAE
R2 = r2_score(y_te, y_pred)
st.markdown(f"### üéØ Hold‚Äëout R¬≤ **{R2:.3f}**")
RMSE = np.sqrt(mean_squared_error(y_te, y_pred))
MAE = mean_absolute_error(y_te, y_pred)

# –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –∏—Ö —Ä–∞—Å—á–µ—Ç–∞

st.caption(f"RMSE **{RMSE:,.0f}**   |   MAE **{MAE:,.0f}**")



# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ SHAP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with st.spinner("SHAP (–ø–æ–¥–≤—ã–±–æ—Ä–∫–∞ 400 —Å—Ç—Ä–æ–∫)‚Ä¶"):
    # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é pipeline
    X_tr_enc = pipe.named_steps["prep"].transform(X_tr)

    # –í—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–µ 400 —Å—Ç—Ä–æ–∫, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –ø—Ä–æ–±–ª–µ–º —Å –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ–º –¥–∞–Ω–Ω—ã—Ö
    sub = np.arange(min(400, X_tr_enc.shape[0]))

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    if X_tr_enc.shape[1] != len(feat_names):
        st.error(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ –º–æ–¥–µ–ª–∏: {X_tr_enc.shape[1]} != {len(feat_names)}")
        st.stop()  # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º SHAP Explainer
    explainer = shap.TreeExplainer(pipe.named_steps["xgb"])
    
    # –í—ã—á–∏—Å–ª—è–µ–º SHAP –∑–Ω–∞—á–µ–Ω–∏—è
    shap_values = explainer.shap_values(X_tr_enc[sub])

# –ü–æ–ª—É—á–∞–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è SHAP –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
shap_abs = np.abs(shap_values).mean(axis=0)
shap_df = pd.DataFrame({"feature": feat_names, "shap": shap_abs}).sort_values("shap", ascending=False)

# Bar plot –¥–ª—è SHAP –≤–∞–∂–Ω–æ—Å—Ç–∏
fig, ax = plt.subplots(figsize=(6, 8))
sns.barplot(y=shap_df.head(20)["feature"], x=shap_df.head(20)["shap"], palette="magma", ax=ax)
ax.set_title("Global SHAP importance")
ax.set_xlabel("|SHAP|")
ax.set_ylabel("")
st.pyplot(fig)

# SHAP beeswarm plot
st.subheader("SHAP beeswarm")
shap.summary_plot(shap_values, pd.DataFrame(X_tr_enc[sub], columns=feat_names),
                  show=False, max_display=25, plot_size=(8, 6))
st.pyplot(plt.gcf())  # –≤—ã–≤–æ–¥ —Ç–µ–∫—É—â–µ–≥–æ —Ä–∏—Å—É–Ω–∫–∞
plt.clf()

# SHAP dependence plots –¥–ª—è —Ç–æ–ø-3 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
st.subheader("SHAP dependence (—Ç–æ–ø‚Äë3)")
for feat in shap_df.head(3)["feature"]:
    shap.dependence_plot(feat, shap_values,
                         pd.DataFrame(X_tr_enc[sub], columns=feat_names),
                         show=False, interaction_index=None, alpha=0.4)
    st.pyplot(plt.gcf())
    plt.clf()






# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Scatter & residuals ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
c1, c2 = st.columns(2)
with c1:
    fig, ax = plt.subplots(figsize=(4,4))
    sns.scatterplot(x=y_te, y=y_pred, ax=ax)
    ax.plot([y_te.min(), y_te.max()], [y_te.min(), y_te.max()], "--k")
    ax.set_xlabel("–§–∞–∫—Ç"); ax.set_ylabel("–ü—Ä–æ–≥–Ω–æ–∑"); ax.set_title("Test: –ø—Ä–æ–≥–Ω–æ–∑ vs —Ñ–∞–∫—Ç")
    st.pyplot(fig)
with c2:
    fig, ax = plt.subplots(figsize=(4,3))
    sns.histplot(y_te - y_pred, bins=30, kde=True, ax=ax)
    ax.set_title("–ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤"); ax.set_xlabel("Residual")
    st.pyplot(fig)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –¢—Ä–µ–Ω–¥—ã –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
st.subheader("–ì–æ–¥–æ–≤—ã–µ —Ç—Ä–µ–Ω–¥—ã (—Ç–æ–ø‚Äë6 —Ä–µ–≥–∏–æ–Ω–æ–≤)")
top_regions = df.groupby(region_col)[target_col].mean().sort_values(ascending=False).head(6).index
for reg in top_regions:
    sub = df[df[region_col]==reg].sort_values(year_col)
    sub_enc = pipe.named_steps["prep"].transform(sub[good_cols + [region_col]])
    sub["pred"] = pipe.named_steps["xgb"].predict(sub_enc)
    fig, ax = plt.subplots(figsize=(5,2.5))
    ax.plot(sub[year_col], sub[target_col], "-o", label="–§–∞–∫—Ç")
    ax.plot(sub[year_col], sub["pred"], "-s", label="–ü—Ä–æ–≥–Ω–æ–∑")
    ax.set_title(reg); ax.set_xlabel("–ì–æ–¥"); ax.set_ylabel("")
    ax.legend(); st.pyplot(fig)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –¢–∞–±–ª–∏—Ü–∞ —Ñ–∞–∫—Ç–æ—Ä–æ–≤ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
stats_rows = []
for feat in shap_df.head(25)["feature"]:
    raw = feat.replace("remainder__", "").replace("cat__", "")
    pearson = df[[raw, target_col]].corr().iloc[0,1] if raw in df else np.nan
    idx = list(feat_names).index(feat)
    stats_rows.append(dict(
        feature=feat,
        pearson_corr=pearson,
        shap_mean=float(shap_values[:, idx].mean()),
        shap_sd=float(shap_values[:, idx].std()),
        nonmiss_ratio=float(df[raw].notna().mean()) if raw in df else np.nan
    ))
stats_df = pd.DataFrame(stats_rows)
st.dataframe(stats_df)

csv = stats_df.to_csv(index=False).encode()
st.download_button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å CSV", csv, "predictor_stats.csv", "text/csv")
